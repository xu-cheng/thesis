\chapter{Literature Review}\label{chap:related-works}

In this chapter, we survey related works on aggregate queries, access control, distributed spatial queries, and authenticated query processing.

\section{Aggregate Queries over Set-Valued Data}

Set-valued data, in which a set of values is associated with an individual, is common in data analytics applications ranging from market basket analysis, to web log mining, to health care research. \citeauthor{10.1145/32204.32219} were the first to extend relational algebra and aggregate functions to set-valued data~\cite{10.1145/32204.32219}. They defined algebraic expressions such as set union, set difference, and aggregation. Since then, data analytics over set-valued data has been extensively studied in various application domains. The most well-known application is association rule mining~\cite{10.1145/170035.170072}. Given a database of sales transactions, each containing a subset of items in the product universe, the objective is to find rules such as ``a user buying item(s) $X$ will probably buy item(s) $Y$''. Although various algorithms are proposed, a fundamental problem in this application is to efficiently compute two aggregate values, namely, the \emph{support} and \emph{confidence} of $X$~\cite{Agrawal:1994:FAM:645920.672836}. The former is the number of transactions that contain $X$, whereas the latter is those in the former that also contain $Y$. With the boom of web search and online advertisement, query log and click stream have become new sources of set-valued data. A variety of aggregate queries have been proposed on these sources for tasks such as website clustering and frequent item identification and counting~\cite{10.14778/2367502.2367508}. Recently, graphs have become another new source of set-valued data. In particular, social networks have contributed various social relations, such as ``friend/unfriend'', ``follow'', ``post/tag'', and user access rights, to set-valued data~\cite{10.1145/1592568.1592585}. Aggregating such data for social network analysis and recommendation has been intensively studied~\cite{10.1145/1367497.1367646}.

\section{Access Control}

Enforcing access control in file systems or database systems has been widely studied in the literature. Traditionally, this is done by employing an \emph{access control list} (ACL), in which a permission manifest is attached to each individual file or data record. However, this method suffers from poor scalability when dealing with massive data or complex access control requirements. To remedy such issues, \emph{role-based access control} (RBAC) is proposed by \citeauthor{10.1109/2.485845}~\cite{10.1109/2.485845}. RBAC implements an access control mechanism over role permissions, user-role, and role-role relationships. This makes it a flexible technology in supporting both \emph{discretionary access control} (DAC) and \emph{mandatory access control} (MAC). However, only static and pre-defined access policies can be supported with RBAC\@. To overcome this limitation and support dynamic, context-aware, and fine-grained access control, \emph{fuzzy identity-based encryption} (Fuzzy IBE)~\cite{10.1007/11426639_27} and, later, \emph{attribute-based encryption} (ABE)~\cite{10.1145/1180405.1180418} are developed. These approaches define an access policy with a complex boolean function over many different attributes to support \emph{attribute-based access control} (ABAC)~\cite{10.1109/ccgrid.2012.92}. There are two categories of ABEs. In \emph{key-policy ABE} (KP-ABE)~\cite{10.1145/1180405.1180418}, each data object is associated with a set of attributes, while users' decryption keys define the access policies with a boolean function over those attributes. In \emph{ciphertext-policy ABE} (CP-ABE)~\cite{10.1109/sp.2007.11}, the access policy is embedded in each data object's ciphertext, while users are given a set of attributes as private keys to present their roles. ABE offers an effective way to enforce fine-grained access control over encrypted data, but it cannot be used to authenticate one's identity. To address this, \emph{attribute-based signature} (ABS)~\cite{10.1007/978-3-642-19074-2_24,10.1145/1755688.1755697} is proposed as a signature scheme to prove one's identity that satisfies certain fine-grained constraints. While ABAC was originally designed for file systems, it has been recently adopted by various cloud databases to support fine-grained access control~\cite{10.1145/2699026.2699101,10.1007/978-3-662-43936-4_21,10.1007/s10916-016-0588-0}.

\section{Distributed Spatial Queries}

There are a large body of research on distributed spatial queries. \citeauthor{10.14778/2536222.2536227} designed the Hadoop-GIS system~\cite{10.14778/2536222.2536227}, which is a spatial data warehousing system and integrates Hive. SpatialHadoop~\cite{10.1109/icde.2015.7113382} adds the traditional spatial indexes to the native Hadoop framework and supports a variety of spatial queries such as range query, kNN, and spatial join. For in-memory computation over cluster machines, GeoSpark~\cite{10.1145/2820783.2820860} has been proposed to support similar spatial queries. More recently, an more efficient in memory spatial analytics system, called Simba~\cite{10.1145/2882903.2915237}, has been proposed. It supports rich spatial queries and has better throughput than that of SpatialHadoop and GeoSpark. Nevertheless, none of the system supports the authenticated query processing to guarantee the integrity of the results.

\section{Authenticated Query Processing}

In general, there are two fundamental approaches to support authenticated query processing, each with its own advantages and disadvantages. On the one hand, one can design a verifiable scheme specifically based on the computation task. This is often achieved by letting the DO sign a well-designed ADS, based on which the SP can construct corresponding proofs for the outsourced computation. This approach yields low overhead but supports only limited computation tasks. On the other hand, a verification scheme may model the computation task as a general Turing machine. As such, they can support arbitrary tasks at the expense of high and sometimes impractical overhead.

For the ADS-based verification schemes, there are two basic techniques: signature chaining (\Cref{fig:related-works:chain}) and Merkle hash tree (\Cref{fig:related-works:mht}). The former technique uses a public-key cipher with which the data owner can generate digital signatures for each data item using a private key. Consequently, the client can verify the authenticity of the data item using the public key. To establish the completeness of the query results, chaining signatures are generated to capture the correlation of each item and its adjacent items~\cite{10.1109/ICDE.2004.1320027}. \emph{Merkle hash trees} (MHTs), on the other hand, are built on index trees~\cite{10.1007/0-387-34805-0_21}. Each entry in a leaf node is assigned a digest based on its hashed value, and each entry in an internal node is assigned a digest derived from its child nodes. The data owner signs the root digest of the MHT, which can be used to verify any subset of data items. For example, in \Cref{fig:related-works:mht}, a range query $Q$ will return $\{o_2, N_2, N_3\}$, based on which the client can reconstruct the root digest $N_0 = H( H(N_3 | H(o_2))| N_2)$ for verification. MHT has been widely adapted to various index structures. Typical examples include the Merkle B-tree for relational data~\cite{10.1145/1142473.1142488}, the Merkle R-tree for spatial data~\cite{10.1007/s00778-008-0113-2,10.1109/icde.2011.5767829}, the authenticated inverted index for text data~\cite{10.14778/1453856.1453875}, the graph metric tree for subgraph similarity search~\cite{10.1109/tkde.2014.2316818}, and the authenticated prefix tree for multi-source data~\cite{10.1145/2723372.2747649}. It has also been adopted to support authenticated join queries~\cite{10.1145/1559845.1559849}, and authenticated privacy-preserving location-based services~\cite{10.1145/2213836.2213871,10.1109/icde.2013.6544932,10.14778/2732219.2732224}. More recently, there have been studies of the ADS schemes on the set-valued data~\cite{10.1007/978-3-642-22792-9_6,10.1007/978-3-642-54631-0_7,10.1109/eurosp.2017.35}. They utilize a cryptographic set accumulator, which is able to present a set with a constant-size digest and support a variety of authenticated set operations such as subset, set disjoint, set sum, and set intersection. Based on that, one can achieve authenticated query processing over high dimensional data~\cite{10.1145/2660267.2660373}, authenticated SQL processing~\cite{10.1145/2810103.2813711}. However, these schemes do not support multiset operations, nor do they consider the aggregate queries over the set-valued data. Further, there is no existing work on authenticated query processing with fine-grained access control or authenticating queries in distributed environment.

\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.5\linewidth}
        \centering
        \resizebox{.7\linewidth}{!}{\input{./figs/sig-chain.tex}}
        \caption{Signature Chaining}\label{fig:related-works:chain}
    \end{minipage}~%
    \begin{minipage}[b]{0.5\linewidth}
        \centering
        \resizebox{\linewidth}{!}{\input{./figs/mht.tex}}
        \caption{Merkle Hash Tree}\label{fig:related-works:mht}
    \end{minipage}
\end{figure}

In comparison, the general-purpose verifiable cloud computing scheme does not assume any specific properties on the computation task. Instead, the computation task is present as a boolean or arithmetic circuit, which is Turing complete and thus can be used for arbitrary cloud computation. As the boolean or arithmetic circuit can be viewed as a serious of constraints on the internal computation state as well as the final results, it can be transformed into a so-called quadratic span program. By utilizing certain cryptographic primitives such as pairing, the equivalent program can be verified using a technique known as zk-SNARKs~\cite{10.1109/sp.2013.47}. In addition to the ability to authenticate arbitrary programs, zk-SNARKs is able to achieve extremely low overhead on verification. The verification time and proof size are both in constant. Further, it leaks no information beyond the computation result, which is crucial for the applications with privacy and confidentiality concerns. However, as a cost of its generality, zk-SNARKs yields high overhead on the preprocessing time and proving time. It is considered impractical to many real-world cloud computing problems. Nevertheless, many studies have been carried out to reduce its proving overhead. \citeauthor{Ben-Sasson:2014:SNZ:2671225.2671275} propose a zk-SNARK variant that avoids hard-coding the computation program into its verification key and thus reduces the preprocessing cost~\cite{Ben-Sasson:2014:SNZ:2671225.2671275}. \citeauthor{10.1109/sp.2017.43} propose an interactive protocol for general-purpose SQL queries, whose cost is substantially lower than the original zk-SNARKs scheme~\cite{10.1109/sp.2017.43}. More recently, it has been proposed to model the computation task as a \emph{random access machine} (RAM) program as opposed to a circuit~\cite{10.1145/2517349.2522733,10.1007/978-3-642-40084-1_6,10.1109/sp.2018.00013}. As a result, it can gain significant performance improvement for some computation tasks. For example, the size of a circuit implementing binary search on a sorted array is linear in the length of the array, whereas the complexity of a RAM program for binary search is only logarithmic.

